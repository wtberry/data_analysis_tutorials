{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Dark Art of Coding:\n",
    "## Introduction to Python\n",
    "Gathering data from the web\n",
    "\n",
    "<img src='../images/dark_art_logo.600px.png' width='300' style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, students should expect to:\n",
    "\n",
    "* Use and understand the basics of the urllib module\n",
    "* Use and understand the basics of the beautiful soup library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCP\n",
    "\n",
    "TCP is a protocol that is used to send data across a network\n",
    "\n",
    "* It relies upon some builtin mechanisms to help increase reliability\n",
    "* TCP creates connections between two devices (connection oriented protocol)\n",
    "* It uses checks to ensure that all data has been sent, if not can request that missing data be resent\n",
    "* TCP organizes packets in order\n",
    "* Between the reliability checks and the organization/ordering of packets, it work very well for sending files (like web pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port numbers\n",
    "\n",
    "The TCP protocol incorporates the use of port numbers:\n",
    "\n",
    "* Port numbers are used by computers to ensure that traffic coming to a given computer gets funneled to the correct application\n",
    "* Multiple ports allow multiple applications on the same computer to talk without interfering with each other\n",
    "* Typically certain applications have default TCP port numbers that are used to send higher-level protocols\n",
    "\n",
    "Task | Port\n",
    ":----|:----\n",
    "Telnet | 23\n",
    "SSH | 22\n",
    "HTTP | 80\n",
    "HTTPS | 443\n",
    "SMTP (E-mail) | 25\n",
    "DNS (Domain Name) | 53\n",
    "FTP (File Transfer) | 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP (Hyper Text Transfer Protocol)\n",
    "\n",
    "HTTP is a common protocol that may be sent using TCP.\n",
    "\n",
    "* HTTP is the standard Protocol for most applications on the internet\n",
    "* Invented to retrieve HTML, images, Documents, etc.\n",
    "* Basic concept:\n",
    "    * Make a connection\n",
    "    * Request a document\n",
    "    * Retrieve the document\n",
    "    * Close the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical Uniform Resource Locator (URL) address has several components:\n",
    "\n",
    "* The URL indicates the protocol, generally HTTP (but it could be others)\n",
    "* It lists the server that hosts the document\n",
    "* The name and path to the document\n",
    "\n",
    "http://  | www.py4e.com | /lessons/network\n",
    ":--------|:-------------|:----------------\n",
    "Protocol | Host         | Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HTTP\n",
    "\n",
    "* Browser attempts to connect to `http://www.example.com`\n",
    "* Issues a request for a document such as `index.html`\n",
    "* The server sends the html document\n",
    "* Browser renders html\n",
    "* Closes connection when done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP requests in Python using urllib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "# First we have to import the request module from urllib\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# urllib allows us to open web pages just like opening files.\n",
    "# The following command creates an http.client.HTTPResponse object that\n",
    "#     gives us access to a number of attributes and behaviors\n",
    "#     related to the data retrieved\n",
    "\n",
    "file = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "# A common technique is to use a for loop to cycle through every\n",
    "# line and print out the data one line at a time\n",
    "# In this case, the data is read in as bytes\n",
    "\n",
    "for line in file:\n",
    "    # We convert each line from bytes to strings using the\n",
    "    #     .decode() attribute.\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arise': 1,\n",
      " 'But': 1,\n",
      " 'It': 1,\n",
      " 'Juliet': 1,\n",
      " 'Who': 1,\n",
      " 'already': 1,\n",
      " 'and': 3,\n",
      " 'breaks': 1,\n",
      " 'east': 1,\n",
      " 'envious': 1,\n",
      " 'fair': 1,\n",
      " 'grief': 1,\n",
      " 'is': 3,\n",
      " 'kill': 1,\n",
      " 'light': 1,\n",
      " 'moon': 1,\n",
      " 'pale': 1,\n",
      " 'sick': 1,\n",
      " 'soft': 1,\n",
      " 'sun': 2,\n",
      " 'the': 3,\n",
      " 'through': 1,\n",
      " 'what': 1,\n",
      " 'window': 1,\n",
      " 'with': 1,\n",
      " 'yonder': 1}\n"
     ]
    }
   ],
   "source": [
    "# Much like other files we have looked at, we can \n",
    "# read and evaluate the text in web-based text files, like\n",
    "# like counting words\n",
    "\n",
    "import pprint\n",
    "import urllib.request\n",
    "file = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "count = {}\n",
    "\n",
    "for line in file:\n",
    "    \n",
    "    # Again, we take the line and use .decode() to convert\n",
    "    #     the data to a string\n",
    "    #     Then we strip the newline\n",
    "    #     Then we split it on spaces\n",
    "    words = line.decode().strip().split()\n",
    "    \n",
    "    # We cycle through the words one at a time\n",
    "    for word in words:\n",
    "        \n",
    "        # If a key for the word already exists .get() grabs the value otherwise it automatically returns 0\n",
    "        count[word] = count.get(word, 0) + 1\n",
    "\n",
    "pprint.pprint(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode and Python text\n",
    "\n",
    "* Internally, within Python 3+, all Python strings are Unicode\n",
    "* When we talk to a network we usually have to encode and decode our data (generally to `utf-8`)\n",
    "* When we recieve data we typically recieve it as a `bytes` object which we then pass through a `.decode()` method to get a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'> b'Who is already sick and pale with grief\\n'\n"
     ]
    }
   ],
   "source": [
    "# Poor man debugging...\n",
    "# This is one of the most important lines of code to any new Pythonista\n",
    "\n",
    "print(type(line), line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Who is already sick and pale with grief\\n'\n",
      "Who is already sick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us look at the difference between outputting:\n",
    "#     a bytes object vs.\n",
    "#     a string\n",
    "\n",
    "print(line)\n",
    "print(line.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading web pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "# Our earlier examples were fairly straightforward, since we \n",
    "#     retrieved text files. Most of the web is not \n",
    "#     straight text files, it is composed of \n",
    "#     Hyper Text Markup Language (HTML)\n",
    "\n",
    "# We request a page using urllib.request.urlopen()\n",
    "\n",
    "page = urllib.request.urlopen('http://dr-chuck.com/page1.htm')\n",
    "\n",
    "for line in page:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful soup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible to use `urllib` to read data from the web, a third party library, `Beautiful Soup` is commonly used instead to supplement urllib. `Beautiful Soup`:\n",
    "\n",
    "* Makes reading and parsing web pages a lot easier\n",
    "* Allows you to extract tags of only certain types\n",
    "* You can find certain tags based on their relationship in the tag heirarchy\n",
    "* Getting hyperlinks becomes a whole lot easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the command line\n",
    "\n",
    "To install Beautiful Soup, you can run this on the command line:\n",
    "\n",
    "*`conda install beautifulsoup4`*\n",
    "\n",
    "## In a Python file/interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "# Get the html text from the HTTPResponse object\n",
    "# Notice the read() method >>>\n",
    "\n",
    "htmlText = urllib.request.urlopen('http://www.unicode.org/').read()\n",
    "\n",
    "# Use bs4 to create a soup object from our html text\n",
    "# Provide a argument to identify which type of parser to\n",
    "#     use, in this case, an html parser\n",
    "\n",
    "soup = BeautifulSoup(htmlText, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The soup object allows you to retrieve specific types of tags, in this\n",
    "#     anchor tags (identified using an 'a'). Anchor tags are used for links.\n",
    "\n",
    "tags = soup('a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.unicode.org/contacts.html\n",
      "http://www.unicode.org/sitemap/\n",
      "http://www.unicode.org/search/\n",
      "\n",
      "http://www.unicode.org/standard/WhatIsUnicode.html\n",
      "http://www.unicode.org/consortium/newcomer.html\n",
      "http://www.unicode.org/glossary/\n",
      "http://www.unicode.org/copyright.html\n",
      "\n",
      "http://www.unicode.org/standard/where/\n",
      "http://www.unicode.org/help/display_problems.html\n",
      "http://www.unicode.org/consortium/distlist.html\n",
      "http://www.unicode.org/resources/\n",
      "http://www.unicode.org/history/\n",
      "http://www.unicode.org/casestudy/\n",
      "http://www.unicode.org/policies/lastresortfont_eula.html\n",
      "http://www.unicode.org/timesens/calendar.html\n",
      "http://www.unicode.org/fr/\n",
      "\n",
      "http://www.unicode.org/education/\n",
      "http://www.unicode.org/education/consortwork.html\n",
      "http://www.unicode.org/education/students.html\n",
      "http://www.unicode.org/education/ngos.html\n",
      "\n",
      "http://www.unicode.org/conference/about-conf.html\n",
      "http://www.unicode.org/standard/tutorial-info.html\n",
      "\n",
      "http://www.unicode.org/faq/\n",
      "http://www.unicode.org/faq/basic_q.html\n",
      "http://www.unicode.org/faq/emoji_dingbats.html\n",
      "http://www.unicode.org/faq/utf_bom.html\n",
      "http://www.unicode.org/faq/font_keyboard.html\n",
      "http://www.unicode.org/faq/indic.html\n",
      "\n",
      "http://www.unicode.org/consortium/consort.html\n",
      "http://www.unicode.org/consortium/memblogo.html\n",
      "http://www.unicode.org/consortium/join.html\n",
      "http://www.unicode.org/policies/policies.html\n",
      "http://www.unicode.org/acknowledgements/\n",
      "http://www.unicode.org/acknowledgements/grants.html\n",
      "http://www.unicode.org/consortium/memoriam.html\n",
      "http://blog.unicode.org/\n",
      "http://www.unicode.org/announcements/\n",
      "http://www.unicode.org/contacts.html\n",
      "\n",
      "http://www.unicode.org/standard/standard.html\n",
      "http://www.unicode.org/versions/latest/\n",
      "http://www.unicode.org/versions/enumeratedversions.html\n",
      "http://www.unicode.org/versions/index.html#References\n",
      "http://www.unicode.org/charts/\n",
      "http://www.unicode.org/ucd/\n",
      "http://www.unicode.org/charts/unihan.html\n",
      "http://www.unicode.org/ivd/\n",
      "http://www.unicode.org/errata/\n",
      "\n",
      "http://cldr.unicode.org/index\n",
      "http://cldr.unicode.org/index/downloads\n",
      "http://cldr.unicode.org/index/survey-tool\n",
      "http://cldr.unicode.org/index/bug-reports\n",
      "http://cldr.unicode.org/index/charts\n",
      "http://cldr.unicode.org/index/process\n",
      "http://www.unicode.org/reports/tr35/\n",
      "http://cldr.unicode.org/index/cldr-spec\n",
      "\n",
      "http://www.unicode.org/publications/\n",
      "http://www.unicode.org/faq/specifications.html\n",
      "http://www.unicode.org/reports/index.html#annexes\n",
      "http://www.unicode.org/reports/index.html#standards\n",
      "http://www.unicode.org/reports/index.html#reports\n",
      "http://www.unicode.org/notes/\n",
      "http://www.unicode.org/onlinedat/online.html\n",
      "\n",
      "http://www.unicode.org/timesens/calendar.html\n",
      "http://www.unicode.org/consortium/utc.html\n",
      "http://www.unicode.org/cldr/process.html\n",
      "http://site.icu-project.org/\n",
      "http://uli.unicode.org/\n",
      "http://www.unicode.org/consortium/edcom.html\n",
      "http://www.unicode.org/iso15924/\n",
      "http://www.unicode.org/ivd/\n",
      "http://www.unicode.org/udhr/\n",
      "http://unicode.org/consortium/tc-procedures.html\n",
      "http://www.unicode.org/wg2/\n",
      "\n",
      "http://www.unicode.org/L2/index.html\n",
      "http://www.unicode.org/L2/L-curdoc.htm\n",
      "http://www.unicode.org/L2/L2search.html\n",
      "http://www.unicode.org/pending/docsubmit.html\n",
      "http://www.unicode.org/L2/meetings/utc-meetings.html\n",
      "http://www.unicode.org/consortium/utc-minutes.html\n",
      "\n",
      "http://www.unicode.org/review/\n",
      "http://www.unicode.org/alloc/Pipeline.html\n",
      "http://www.unicode.org/roadmaps\n",
      "http://www.unicode.org/pending/docsubmit.html\n",
      "http://www.unicode.org/pending/proposals.html\n",
      "http://unicode.org/reporting.html\n",
      "http://www.unicode.org/members/index.html\n",
      "http://www.unicode.org/press/\n",
      "http://www.unicode.org/contacts.html\n",
      "http://blog.unicode.org/\n",
      "http://blog.unicode.org/feeds/posts/default?alt=rss\n",
      "http://www.facebook.com/pages/Friends-of-Unicode/127785250588285\n",
      "http://twitter.com/unicode/\n",
      "https://plus.google.com/109412260435993059737\n",
      "http://www.unicode.org/consortium/donations.html\n",
      "http://www.unicode.org/consortium/memblogo.html#indiv\n",
      "http://www.unicode.org/timesens/calendar.html#nextUTC\n",
      "http://www.unicode.org/timesens/calendar.html\n",
      "http://www.unicode.org/charts/\n",
      "http://www.unicode.org/versions/latest/\n",
      "http://cldr.unicode.org/\n",
      "http://www.unicode.org/L2/L-curdoc.htm\n",
      "http://site.icu-project.org/\n",
      "http://www.unicode.org/timesens/calendar.html\n",
      "http://blog.unicode.org/\n",
      "http://www.unicodeconference.org/uni\n",
      "http://www.unicode.org/consortium/adopt-a-character.html\n",
      "http://www.unicode.org/consortium/adopt-a-character.html\n",
      "http://blog.unicode.org/\n",
      "http://blog.unicode.org/2017/05/unicode-emoji-submission-deadline-now.html\n",
      "http://blog.unicode.org/2017/04/call-for-unicode-100-cover-design-art.html\n",
      "http://blog.unicode.org/2017/05/unicode-emoji-50-specification-now-final.html\n",
      "http://blog.unicode.org/2017/04/icu-59-released.html\n",
      "http://blog.unicode.org/2017/03/cldr-version-31-released.html\n",
      "http://www.unicode.org/review/\n",
      "http://www.unicode.org/review/#aboutPRI\n",
      "http://www.unicode.org/ivd/pri/pri351/\n",
      "http://unicode.org/versions/beta-10.0.0.html\n",
      "http://www.unicode.org/ivd/pri/pri349/\n",
      "http://www.unicode.org/copyright.html\n"
     ]
    }
   ],
   "source": [
    "# Let's cycle through the tags and get the 'href' data portion. this is the data that contains the link itself\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using documentation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the documentation for a third party library.\n",
    "\n",
    "The documentation for Beautiful Soup has a number of nice attributes that can get you started fairly quickly, so let's use the documentation to enhance our knowledge of the subject.\n",
    "\n",
    "[Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "---\n",
    "\n",
    "## What is web scraping?\n",
    "\n",
    "Web scraping is a technique used to retrieve data from the web OR from similar networks (intranets, etc).\n",
    "\n",
    "* Web scrapers simulate the behavior of a browser\n",
    "* They look at the data from specific site(s)\n",
    "* They extract specific information you need from it\n",
    "* Typically this is done over and over again across multiple sites\n",
    "\n",
    "## Why web scrape?\n",
    "\n",
    "* Get data from a sites that don't provide mechanisms to export the data\n",
    "* Collect information on sites to build a search engine database\n",
    "* Monitor sites for changes\n",
    "* Collect social network data\n",
    "    * who is connected to or communicates with who?\n",
    "    * What is being said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source:\n",
    "# http://www.jabberwocky.com/carroll/jabber/jabberwocky.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
