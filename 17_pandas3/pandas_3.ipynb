{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Dark Art of Coding:\n",
    "## Introduction to Python\n",
    "Data Handling\n",
    "\n",
    "<img src='../images/dark_art_logo.600px.png' width='300' style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, students should expect to:\n",
    "\n",
    "* Merge DataFrames effectively\n",
    "* Unstack your data\n",
    "* Replace unwanted data with better versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with two data sets. One is shorter than the other but generally they're similar. Both have a column with names and a column with countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readers1 = pd.read_csv('reader_stats.csv')\n",
    "readers2 = pd.read_csv('reader_stats_short.csv')\n",
    "\n",
    "print(\"readers1 data:\", '\\n', readers1)\n",
    "print('-' * 40)\n",
    "print(\"readers2 data:\", '\\n', readers2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built in functions of pandas let us merge two data frames together in multiple different ways. These merges are similar to the ones you might see in a SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readerso = pd.merge(readers1, readers2, how='outer')\n",
    "readersi = pd.merge(readers1, readers2, how='inner')\n",
    "readersl = pd.merge(readers1, readers2, how='left')\n",
    "readersr = pd.merge(readers1, readers2, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Base.jpg' width='600' style='float:center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Outer Join\\n')\n",
    "readerso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Outer.jpg' width='600' style='float:center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Inner Join\\n')\n",
    "readersi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Inner.jpg' width='600' style='float:center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Left Join\\n')\n",
    "readersl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Left.jpg' width='600' style='float:center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Right Join\\n')\n",
    "readersr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Right.jpg' width='600' style='float:center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Please be aware, that unless you specify otherwise, these joins are based on the contents of the entire row. In many cases, we simply want to join based on the contents of one or more columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key columns\n",
    "Remember, DataFrames can be built from dictionaries, using the keys of the dictionary as the source of the column in the DataFrame. Any elements (stored as a sequence) in the values associated with those keys then become the elements in the respective column\n",
    "\n",
    "Here, we are creating some **key** columns that we can use to create joins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfa = DataFrame({'key':     ['bruce', 'bruce', 'diana', 'bruce', 'hal', 'diana', 'kara'],\n",
    "                 'emails_left': [112, 111, 201, 109, 113, 203, 204]}) \n",
    "\n",
    "dfb = DataFrame({'key':        ['hal', 'bruce', 'selina', 'diana'],\n",
    "                 'ages_right': [36, 37, 33, 34]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imagine, that using the previous data, we wanted to do an analysis of emails versus\n",
    "# age (i.e. whether age impacts the number of emails someone receives over time).\n",
    "# Let's start with a Left Join: \n",
    "\n",
    "dfl = pd.merge(dfa, dfb, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's look at an Inner Join:\n",
    "\n",
    "dfi = pd.merge(dfa, dfb, on='key', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple key columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, again, we create a set of DataFrames based on dictionaries.\n",
    "# This time we choose to use more than one column that will be used as keys to\n",
    "# match data in each of the DataFrames.\n",
    "\n",
    "\n",
    "dfa = DataFrame({'fname_key': ['bruce', 'bruce', 'hal', 'selina', 'hal'],\n",
    "                 'lname_key': ['wayne', 'jordan', 'wayne', 'kyle', 'jordan'],\n",
    "                 'ages_left': [37, 53, 54, 33, 36]})\n",
    "\n",
    "dfb = DataFrame({'fname_key': ['hal', 'bruce', 'hal', 'kara', 'hal'],\n",
    "                 'lname_key': ['jordan', 'wayne', 'jordan', 'zor-el', 'jordan'],\n",
    "                 'emails_right': [189, 111, 193, 253, 187]})\n",
    "\n",
    "# Outer Join \n",
    "dfo = pd.merge(dfa, dfb, on=['fname_key', 'lname_key'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "dfi = pd.merge(dfa, dfb, on=['fname_key', 'lname_key'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Points!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In your **text editor** create a simple script called:\n",
    "\n",
    "```bash\n",
    "my_merge_01.py```\n",
    "\n",
    "Execute your script in the **IPython interpreter** using the command:\n",
    "\n",
    "```bash\n",
    "run my_merge_01.py```\n",
    "\n",
    "Your script should do the following:\n",
    "* Read in two csv files (Don't worry about column names. the files have a header row that is turned into column names for you):\n",
    "    * `left_file.csv`\n",
    "    * `right_file.csv`\n",
    "* Merge the two DataFrames using the `name` column as the key and using an inner join\n",
    "* Create a new column called `matchip` of True/False values where the `toip` column and the `fromip` column match\n",
    "* Output just the rows where `matchip` is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you complete this exercise, please put your green post-it on your monitor. \n",
    "\n",
    "If you want to continue on at your own-pace, please feel free to do so.\n",
    "\n",
    "<img src='../images/green_sticky.300px.png' width='200' style='float:left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.read_csv('left_file.csv')\n",
    "dfr = pd.read_csv('right_file.csv')\n",
    "comb = pd.merge(dfl, dfr, on='name', how='inner')\n",
    "comb['matchip'] = comb.fmip == comb.toip\n",
    "comb[comb.matchip]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Series/DataFrames (like some of the other data we've handled) can concatenate. However instead of using the `+` like with lists or strings. You have to use Pandas built in function `pd.concat()`. The default behaviour is to stack the data end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = Series(['wayne', 'jordan'], index=[1, 2])\n",
    "names2 = Series(['dinah', 'kent'], index=[4, 5])\n",
    "names3 = Series(['rayner', 'gordon', 'grayson'], index=[6, 7, 8])\n",
    "\n",
    "pd.concat([names1, names3, names2], axis=0)\n",
    "\n",
    "# An alternate method is to stack columns side by side\n",
    "# pd.concat([names1, names3, names2], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names4 = pd.concat([names1, names3])\n",
    "pd.concat([names1, names4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([names1, names3, names3], keys=['rho', 'sigma', 'tau'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([names1, names3, names3], axis=1, keys=['rho', 'sigma', 'tau'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prep our next data set, we'll use yet another way to generate DataFrames...\n",
    "# These nested lists will become the rows in our DataFrame\n",
    "# AS a reminder, you can assign columns when you generate the Frame\n",
    "# If you don't have any need for the original indexes, you can ignore\n",
    "# them and pandas will auto-generate an brand-new index on the fly when you do a \n",
    "# concatenation.\n",
    "\n",
    "dfa = DataFrame([[11, 21, 31, 41],\n",
    "                 [13, 25, 32, 49],\n",
    "                 [11, 21, 31, 41],\n",
    "                 [11, 21, 31, 42]], columns=['iota', 'kappa', 'lambda', 'mu'])\n",
    "\n",
    "dfb = DataFrame([[55, 66, 77],\n",
    "                 [53, 63, 73]], columns=['kappa', 'lambda', 'mu'])\n",
    "\n",
    "print(dfa)\n",
    "print(dfb)\n",
    "\n",
    "pd.concat([dfa, dfb], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstacking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When generating DataFrames, another common method, especially with ranges of\n",
    "# data OR with randomized data is to use functions in numpy to seed\n",
    "# the Frame with ranges and/or randomized values. \n",
    "# Here, we are creating a Frame with the numbers 100 to 114 and shaping it to be a \n",
    "# three by five table.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df = DataFrame(np.arange(100, 115).reshape((3, 5)),\n",
    "               index=pd.Index(['kara', 'dinah', 'selina'], name='justiceleague'),\n",
    "               columns=pd.Index(['wed', 'thu', 'fri', 'sat', 'sun'], name='day'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default level to unstack is the innermost\n",
    "df.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.unstack()\n",
    "s['wed']['kara']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can refer to the level to unstack by an integer number, starting\n",
    "# with the farthest left being noted as 0. By default, pandas unstacks from the \n",
    "# innermost level of a multi-level hierarchical index.\n",
    "\n",
    "# The following code comes directly from the pandas documentation:\n",
    "# http://pandas.pydata.org/pandas-docs/stable/advanced.html\n",
    "# Several take-aways for this code... \n",
    "#     * use the documentation > plenty of great examples are in there.\n",
    "#     * This MultiIndex dataframe is a nice setup for demoing multilevel unstacking\n",
    " \n",
    "'''\n",
    "In [1]: arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n",
    "   ...:           ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n",
    "   ...: \n",
    "In [2]: tuples = list(zip(*arrays))\n",
    "In [3]: tuples\n",
    "Out[3]: \n",
    "[('bar', 'one'),\n",
    " ('bar', 'two'),\n",
    " ('baz', 'one'),\n",
    " ('baz', 'two'),\n",
    " ('foo', 'one'),\n",
    " ('foo', 'two'),\n",
    " ('qux', 'one'),\n",
    " ('qux', 'two')]\n",
    "In [4]: index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n",
    "In [5]: index\n",
    "Out[5]: \n",
    "MultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two']],\n",
    "           labels=[[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 0, 1, 0, 1, 0, 1]],\n",
    "           names=['first', 'second'])\n",
    "In [6]: s = pd.Series(np.random.randn(8), index=index)\n",
    "In [7]: s\n",
    "Out[7]: \n",
    "first  second\n",
    "bar    one       0.469112\n",
    "       two      -0.282863\n",
    "baz    one      -1.509059\n",
    "       two      -1.135632\n",
    "foo    one       1.212112\n",
    "       two      -0.173215\n",
    "qux    one       0.119209\n",
    "       two      -1.044236\n",
    "dtype: float64\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n",
    "          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n",
    "\n",
    "tuples = list(zip(*arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.random.randn(8), index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the example above, it is possible to demonstrate several levels of unstacking.\n",
    "# As noted, the default level of unstacking is to unstack from the innermost level\n",
    "# of a MultiIndex. Levels are numbered started at the outermost level being '0' and \n",
    "# incrementing as they move inward.\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.unstack(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.unstack(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can refer to the level to unstack by the name of the Index.\n",
    "\n",
    "s.unstack('second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another great tool for looking at your data in more convenient ways is to use a \n",
    "# pivot table. Let's start with a DataFrame that has three columns based on \n",
    "# this list of lists. A timestamp, a Justice League hero and the number of \n",
    "# Tweets they received on a given day.\n",
    "\n",
    "league = DataFrame([['2016-03-10T00:00:00', 'jordan', 221],\n",
    "                    ['2016-03-10T00:00:00', 'wayne', 222],\n",
    "                    ['2016-03-10T00:00:00', 'kyle', 345],\n",
    "                    ['2016-03-11T00:00:00', 'jordan', 222],\n",
    "                    ['2016-03-11T00:00:00', 'wayne', 223],\n",
    "                    ['2016-03-11T00:00:00', 'kyle', 323],\n",
    "                    ['2016-03-12T00:00:00', 'jordan', 201],\n",
    "                    ['2016-03-12T00:00:00', 'wayne', 209],\n",
    "                    ['2016-03-12T00:00:00', 'kyle', 340],\n",
    "                    ['2016-03-13T00:00:00', 'jordan', 220],\n",
    "                    ['2016-03-13T00:00:00', 'wayne', 223],\n",
    "                    ['2016-03-13T00:00:00', 'kyle', 339],\n",
    "                    ['2016-03-14T00:00:00', 'jordan', 201],\n",
    "                    ['2016-03-14T00:00:00', 'wayne', 219],\n",
    "                    ['2016-03-14T00:00:00', 'kyle', 345]],\n",
    "                    columns=['timestamp', 'jleague', 'tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the league DataFrame, we can create a pivot table using the pivot() command:\n",
    "\n",
    "tweet_view = league.pivot('timestamp', 'jleague', 'tweets')\n",
    "tweet_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league['fan_index'] = abs(np.random.randn(len(league)))\n",
    "league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_view2 = league.pivot('timestamp', 'jleague')\n",
    "tweet_view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_view2['fan_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicates and replacing values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dfd = dfa\n",
    "dfd['zeta'] = [4, 1, 4, 1]\n",
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.duplicated(['iota', 'kappa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .map()\n",
    "\n",
    "# legend:\n",
    "# 0 = 'm'\n",
    "# 1 = 'f'\n",
    "\n",
    "genders = {'selina kyle': '1',\n",
    "           'bruce wayne': '0',\n",
    "           'dinah lance': '1',\n",
    "           'hal jordan': '0',\n",
    "           'clark kent': '0',\n",
    "           'barry allen': '0',\n",
    "           'arthur curry': '0',\n",
    "           'billy batson': '0',\n",
    "           'barbara gordon': '1',\n",
    "           'kara zor-el': '1',\n",
    "           'john jones': '0',\n",
    "           'diana prince': '1',\n",
    "           'dick grayson': '0',\n",
    "           'john jones': '0',\n",
    "           'victor stone': '0',\n",
    "           'ray palmer': '0',\n",
    "           'john constantine': '0',\n",
    "           'kyle rayner': '0',\n",
    "           'wally west': '0'}\n",
    "\n",
    "\n",
    "it = pd.read_csv('ig_tweets.csv')\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uses a dictionary to map keys to values\n",
    "\n",
    "it['gender'] = it['jleague'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run a function on the entire series using apply\n",
    "\n",
    "it['jleagueLower'] = it['jleague'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it['gender'] = it['jleagueLower'].map(genders)\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_conv(name):\n",
    "    gen = genders[name.lower()]\n",
    "    if gen == '0':\n",
    "        return 'm'\n",
    "    elif gen == '1':\n",
    "        return 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it['gender'] = it['jleague'].apply(gen_conv)\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also replace certain values wholesale if desired, using the replace() function\n",
    "# Using .replace()\n",
    "it.gender.replace('f', 'Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it.gender.replace(['f', 'm'], ['Female', 'Male'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bins\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = it.tweets\n",
    "bins = [2, 5, 9, 15]\n",
    "\n",
    "categories = pd.cut(msgs, bins)\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math notation ... '(' open   OR exclusive\n",
    "#                   ']' closed OR inclusive\n",
    "# 2 < x <= 5        (2, 5]\n",
    "\n",
    "#                   right=True/False\n",
    "\n",
    "pd.value_counts(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['few', 'medium', \"aren't there bad guys to catch\"]\n",
    "it['workload'] = pd.cut(it.tweets, bins, labels=labels)\n",
    "it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experience Points!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In your **text editor** create a simple script called:\n",
    "\n",
    "```bash\n",
    "my_bin_01.py\n",
    "```\n",
    "\n",
    "Execute your script in the **IPython interpreter** using the command:\n",
    "\n",
    "```bash\n",
    "run my_bin_01.py\n",
    "```\n",
    "\n",
    "Your script should do the following:\n",
    "\n",
    "* Bring out your merged DataFrame from the last exercise\n",
    "* Bin the payload column by 100_000 increments up to AND INCLUDING 1_000_000 with labels where you spell it out E.G.\n",
    "    * `One hundred thousand`\n",
    "    * `Two hundred thousand`\n",
    "    * ...\n",
    "    * `Nine hundred thousand`\n",
    "    * `One million`\n",
    "* Store the binned data in a new column called `bins`\n",
    "* Create a pivot table using:\n",
    "    * `lat` column as index\n",
    "    * `long` column as column names\n",
    "    * `bins` column as values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you complete this exercise, please put your green post-it on your monitor. \n",
    "\n",
    "If you want to continue on at your own-pace, please feel free to do so.\n",
    "\n",
    "<img src='../images/green_sticky.300px.png' width='200' style='float:left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.lat = comb.lat.apply(int)\n",
    "comb.long = comb.long.apply(int)\n",
    "\n",
    "comb['bins'] = pd.cut(comb.payload, range(0, 1000001, 100000), labels=['one hundred thousand',\n",
    "                                                                       'two hundred thousand',\n",
    "                                                                       'three hundread thousand',\n",
    "                                                                       'four hundred thousand',\n",
    "                                                                       'five hundred thousand',\n",
    "                                                                       'six hundred thousand',\n",
    "                                                                       'seven hundred thousand',\n",
    "                                                                       'eight hundred thousand',\n",
    "                                                                       'nine hundred thousand',\n",
    "                                                                       'one million'])\n",
    "\n",
    "comb.pivot('lat', 'long', 'bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
